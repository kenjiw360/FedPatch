{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52564f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import socket\n",
    "import threading\n",
    "import struct\n",
    "import pickle\n",
    "import time\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07073d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client():\n",
    "\tdef __init__(self, model, loss_fn, dataset, args):\n",
    "\t\tself.args = args\n",
    "\n",
    "\t\tself.model = model\n",
    "\t\tself.loss_fn = loss_fn\n",
    "\t\tself.dataset = dataset\n",
    "\t\tself.train_loader = torch.utils.data.DataLoader(self.dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "\t\tself.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\t\tself.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "\n",
    "\t\tself.server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\t\tself.server.bind((self.socket.getsockname()[0], 0))\n",
    "\t\tself.server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "\n",
    "\t\tself.government = None\n",
    "\n",
    "\t\tself.update = None\n",
    "\t\tself.round = 0\n",
    "\n",
    "\tdef to(self, device):\n",
    "\t\tself.device = device\n",
    "\t\t\n",
    "\t\tself.model.to(self.device)\n",
    "\t\tself.loss_fn.to(self.device)\n",
    "\n",
    "\tdef connect(self, host=\"127.0.0.1\", port=65432):\n",
    "\t\tself.government = (host, port)\n",
    "\n",
    "\t\tself.socket.connect(self.government)\n",
    "\n",
    "\t\tprint(\"Fetching Model Size... \", end=\"\")\n",
    "\n",
    "\t\tdata_size = b''\n",
    "\t\twhile len(data_size) < 4:\n",
    "\t\t\tdata_size += self.socket.recv(4 - len(data_size))\n",
    "\t\tdata_size = struct.unpack(\">I\", data_size)[0]\n",
    "\t\t\n",
    "\t\tprint(f\"Done! ({data_size/1024/1024:.1f}MB)\")\n",
    "\n",
    "\t\tprint(\"Fetching Model Weights... \", end=\"\")\n",
    "\n",
    "\t\tbuffer = io.BytesIO()\n",
    "\t\twhile data_size != 0:\n",
    "\t\t\tdata = self.socket.recv(1024)\n",
    "\t\t\tbuffer.write(data)\n",
    "\t\t\tdata_size -= len(data)\n",
    "\n",
    "\t\tbuffer.seek(0)\n",
    "\t\tloaded = torch.load(buffer, weights_only=True)\n",
    "\t\tself.model.load_state_dict(loaded)\n",
    "\n",
    "\t\tprint(\"Done!\")\n",
    "\n",
    "\t\tprint(\"Fetching Round Number... \", end=\"\")\n",
    "\n",
    "\t\tself.socket.sendall(b\"Round Number\")\n",
    "\t\tself.round = b''\n",
    "\t\twhile len(self.round) < 4:\n",
    "\t\t\tself.round += self.socket.recv(4 - len(self.round))\n",
    "\t\tself.round = struct.unpack(\">I\", self.round)[0]\n",
    "\n",
    "\t\tprint(\"Done!\")\n",
    "\n",
    "\tdef train(self):\n",
    "\t\tprint(\"Beginning Training...\")\n",
    "\n",
    "\t\tself.model.train()\n",
    "\t\t\n",
    "\t\tcurrent_lr = self.args[\"lr\"]\n",
    "\t\toptimizer = torch.optim.SGD(self.model.parameters(), lr=current_lr * (self.args[\"lr_decay\"]) ** self.round, weight_decay=self.args[\"w_decay\"])\n",
    "\n",
    "\t\tfor local_epoch in range(self.args[\"num_epochs\"]):\n",
    "\t\t\tstart = time.time()\n",
    "\t\t\tprint(f\"Epoch {local_epoch+1}/{self.args['num_epochs']}... \", end=\"\")\n",
    "\n",
    "\t\t\tfor i, (inputs, labels) in enumerate(self.train_loader):\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tinputs, labels = inputs.to(device=self.device, non_blocking=True), labels.to(device=self.device, non_blocking=True)\n",
    "\t\t\t\toutputs = self.model(inputs)\n",
    "\t\t\t\tminibatch_loss = self.loss_fn(outputs, labels)\n",
    "\t\t\t\tminibatch_loss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\tend = time.time()\n",
    "\t\t\ttrain_time = end - start\n",
    "\t\t\tprint(f\"Done! ({train_time:.1f}s)\")\n",
    "\n",
    "\t\tself.finish_training()\n",
    "\n",
    "\tdef finish_training(self):\n",
    "\t\tself.socket.sendall(f\"Training Complete, {len(self.dataset)}, {self.server.getsockname()}\".replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").encode()) # Notify Server That Training Is Complete\n",
    "\t\tdata = self.socket.recv(1024)\n",
    "\t\tdecoded = data.decode()\n",
    "\n",
    "\t\tif \"Decentralised Aggregation\" not in decoded: return self.disconnect()\n",
    "\n",
    "\t\tbuffer_info = [pair.split(\"#\") for pair in decoded.replace(\"Decentralised Aggregation: \",\"\").split(\", \")]\n",
    "\t\tfor i in range(len(buffer_info)):\n",
    "\t\t\tbuffer_info[i][0] = int(buffer_info[i][0])\n",
    "\t\t\tbuffer_info[i][1] = buffer_info[i][1].split(\":\")\n",
    "\t\t\tbuffer_info[i][1][1] = int(buffer_info[i][1][1])\n",
    "\t\t\tbuffer_info[i][1] = tuple(buffer_info[i][1])\n",
    "\t\t\tbuffer_info[i][2] = buffer_info[i][2].split(\":\")\n",
    "\t\t\tbuffer_info[i][2][1] = int(buffer_info[i][2][1])\n",
    "\t\t\tbuffer_info[i][2] = tuple(buffer_info[i][2])\n",
    "\t\t\tprint(\"Client In Buffer: \", buffer_info[i])\n",
    "\n",
    "\t\tself.aggregate(buffer_info)\n",
    "\t\t\n",
    "\tdef aggregate(self, buffer_info):\n",
    "\t\tprint(\"Beginning Aggregation...\")\n",
    "\t\tself_info = self.socket.getsockname()\n",
    "\t\tid = -1\n",
    "\t\tfor i, [N, client, server] in enumerate(buffer_info):\n",
    "\t\t\tif client == self_info:\n",
    "\t\t\t\tid = i\n",
    "\t\t\t\tbreak\n",
    "\t\tif id == -1:\n",
    "\t\t\tprint(\"Client Is Not Member Of Buffer\")\n",
    "\t\t\treturn self.disconnect()\n",
    "\n",
    "\t\tprint(f\"Generating Patches... \", end=\"\")\n",
    "\n",
    "\t\tnum_patches = len(buffer_info)\n",
    "\t\tpatches = [OrderedDict() for i in range(num_patches)]\n",
    "\n",
    "\t\tparams = self.model.state_dict()\n",
    "\t\t\n",
    "\t\tfor key in params.keys():\n",
    "\t\t\tparams_flat = params[key].flatten()\n",
    "\t\t\tlength = int(params_flat.size(0) / num_patches)\n",
    "\t\t\tfor index in range(num_patches):\n",
    "\t\t\t\tstarting_index = length * index\n",
    "\t\t\t\tending_index = params_flat.size(0) if num_patches - 1 == index else starting_index + length\n",
    "\t\t\t\tpatches[index][key] = params_flat[starting_index:ending_index].clone()\n",
    "\t\t\n",
    "\t\tprint(\"Done!\")\n",
    "\n",
    "\t\tprint(f\"Handling Patch {id}...\")\n",
    "\t\t\n",
    "\t\tself.disconnect()\n",
    "\n",
    "\t\tpatches_server_thread = threading.Thread(target=self.patches_server, args=(id, buffer_info, patches))\n",
    "\t\tpatches_server_thread.start()\n",
    "\n",
    "\t\tself.compute_patch(id, buffer_info, patches[id])\n",
    "\n",
    "\t\tpatches_server_thread.join()\n",
    "\n",
    "\tdef patches_server(self, id, buffer_info, patches):\n",
    "\t\tcompleted_buffer = [id]\n",
    "\n",
    "\t\tself.server.listen()\n",
    "\n",
    "\t\tthreads = []\n",
    "\n",
    "\t\twhile len(completed_buffer) != len(buffer_info):\n",
    "\t\t\tclient, address = self.server.accept()\n",
    "\n",
    "\t\t\tindex = -1\n",
    "\t\t\tfor i, [N, identity, server] in enumerate(buffer_info):\n",
    "\t\t\t\tif address == identity and i not in completed_buffer:\n",
    "\t\t\t\t\tcompleted_buffer.append(i)\n",
    "\t\t\t\t\tindex = i\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\tif index == -1:\n",
    "\t\t\t\tprint(\"Client Not In Buffer...\")\n",
    "\t\t\t\tclient.close()\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tthreads.append(threading.Thread(target=self.patches_server_helper, args=(client, patches[index])))\n",
    "\t\t\tthreads[-1].start()\n",
    "\n",
    "\t\tfor thread in threads: thread.join()\n",
    "\n",
    "\t\tself.server.close()\n",
    "\n",
    "\tdef patches_server_helper(self, client, patch):\n",
    "\t\tbytes = pickle.dumps(patch)\n",
    "\t\tclient.sendall(struct.pack('>I', len(bytes)))\n",
    "\t\tclient.sendall(bytes)\n",
    "\t\n",
    "\tdef compute_patch(self, id, buffer_info, patch):\n",
    "\t\tprint(f\"Should Compute Patch {id}...\")\n",
    "\t\tself.disconnect()\n",
    "\n",
    "\t\tfor key in patch.keys():\n",
    "\t\t\tpatch[key] *= buffer_info[id][0]\n",
    "\n",
    "\t\tfor i, [N, client_sock, server_sock] in enumerate(buffer_info):\n",
    "\t\t\tif id == i: continue\n",
    "\t\t\tprint(f\"Fetching Client {i}'s Model Patch {id}: \", end=\"\\r\")\n",
    "\t\t\tself.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\t\t\tself.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "\t\t\tself.socket.bind(buffer_info[id][1])\n",
    "\n",
    "\t\t\twhile True:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tself.socket.connect(server_sock)\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprint(\"Waiting To Connect...\")\n",
    "\t\t\t\t\ttime.sleep(0.5)\n",
    "\n",
    "\t\t\tdata_size = b''\n",
    "\t\t\twhile len(data_size) < 4:\n",
    "\t\t\t\tdata_size += self.socket.recv(4 - len(data_size))\n",
    "\t\t\tdata_size = struct.unpack(\">I\", data_size)[0]\n",
    "\n",
    "\t\t\tprint(f\"Fetching Client {i}'s Model Patch {id} (0.0MB/{data_size/1024/1024:.1f}MB): [{' ' * 20}]\", end=\"\\r\")\n",
    "\n",
    "\t\t\tresult = b\"\"\n",
    "\n",
    "\t\t\twhile len(result) < data_size:\n",
    "\t\t\t\tdata = self.socket.recv(1024)\n",
    "\t\t\t\tresult += data\n",
    "\t\t\t\tnum_equals = int(len(result)/data_size*20)\n",
    "\t\t\t\tprint(f\"Fetching Client {i}'s Model Patch {id} ({len(result)/1024/1024:.1f}MB/{data_size/1024/1024:.1f}MB): [{'=' * num_equals + ' ' * (20 - num_equals)}]\", end=\"\\r\")\n",
    "\t\t\t\n",
    "\t\t\treceived_patch = pickle.loads(result)\n",
    "\n",
    "\t\t\tprint(end='\\x1b[2K')\n",
    "\t\t\tprint(f\"Fetching Client {i}'s Model Patch {id}: Done!\")\n",
    "\n",
    "\t\t\tfor key in patch.keys():\n",
    "\t\t\t\tpatch[key] += received_patch[key] * N\n",
    "\n",
    "\t\t\tself.disconnect()\n",
    "\n",
    "\t\tN = int(sum([N for (N, client, server) in buffer_info]))\n",
    "\n",
    "\t\tfor key in patch.keys():\n",
    "\t\t\tpatch[key] = patch[key]/N\n",
    "\n",
    "\t\tprint(f\"Finished Computing Patch {id}!\")\n",
    "\t\tprint(f\"Sending Server Patch {id}...\")\n",
    "\n",
    "\t\tself.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\t\tself.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "\t\tself.socket.bind(buffer_info[id][1])\n",
    "\n",
    "\t\tbytes = pickle.dumps(patch)\n",
    "\n",
    "\t\twhile True:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tself.socket.connect(self.government)\n",
    "\t\t\t\tbreak\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(\"Waiting To Connect...\")\n",
    "\t\t\t\ttime.sleep(0.5)\n",
    "\n",
    "\t\tprint(\"Connected To Server!\")\n",
    "\n",
    "\t\tself.socket.sendall(struct.pack('>I', len(bytes)))\n",
    "\t\tself.socket.sendall(bytes)\n",
    "\n",
    "\t\tself.socket.close()\n",
    "\n",
    "\tdef disconnect(self):\n",
    "\t\tself.socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bde8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18()\n",
    "model.fc = nn.Linear(in_features=512, out_features=10, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_num = int(input(\"Select Unique Agent Number (0-8): \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a1aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator()\n",
    "generator.manual_seed(42) # The Universe is simply one massive equivalence statement where both sides of the equation are equal to a constant k, where k = 42\n",
    "\n",
    "dataset_length = len(trainset)\n",
    "\n",
    "client_datasets = random_split(dataset=trainset, lengths=[dataset_length - 8*int(dataset_length/9) if i == 8 else int(dataset_length/9) for i in range(9)], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84964b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(model, loss_fn, client_datasets[agent_num], {\n",
    "\t\"lr\": 0.1,\n",
    "\t\"lr_decay\": 0.99,\n",
    "\t\"w_decay\": 1e-4,\n",
    "\t\"num_epochs\": 2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77256c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.to(torch.device(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67dd31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a7986",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
